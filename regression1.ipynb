{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1. The difference between simple linear regression and multiple linear regression is that **simple linear regression** uses only one independent variable to predict a dependent variable, while **multiple linear regression** uses more than one independent variable to predict a dependent variable¹²³. For example, if you want to predict the height of a person based on their age, you can use simple linear regression. But if you want to predict the height of a person based on their age, gender, and weight, you need to use multiple linear regression.\n",
    "\n",
    "Q2. The assumptions of linear regression are:\n",
    "\n",
    "- **Linearity**: the relationship between the independent and dependent variables is linear.\n",
    "- **Independence**: the observations are independent of each other.\n",
    "- **Homoscedasticity**: the variance of the error terms is constant across the values of the independent variables.\n",
    "- **Normality**: the error terms are normally distributed⁴.\n",
    "\n",
    "You can check whether these assumptions hold in a given dataset by using various methods, such as:\n",
    "\n",
    "- Plotting scatterplots and residual plots to check for linearity and homoscedasticity.\n",
    "- Performing statistical tests such as Durbin-Watson test, Breusch-Pagan test, or Goldfeld-Quandt test to check for independence and homoscedasticity.\n",
    "- Performing statistical tests such as Shapiro-Wilk test, Kolmogorov-Smirnov test, or Jarque-Bera test to check for normality.\n",
    "\n",
    "Q3. The slope and intercept in a linear regression model represent the **rate of change** and the **starting point** of the relationship between the independent and dependent variables, respectively⁵. For example, if you have a linear regression model that predicts the sales of a product based on its price, the slope tells you how much the sales change for every unit change in price, and the intercept tells you what the sales would be if the price was zero.\n",
    "\n",
    "Q4. Gradient descent is an optimization algorithm that finds the **minimum value** of a function by iteratively updating its parameters in the opposite direction of the gradient (the slope) of the function⁵. It is used in machine learning to find the optimal parameters of a model that minimize the loss function (the difference between the predicted and actual values).\n",
    "\n",
    "Q5. The multiple linear regression model is a generalization of the simple linear regression model that allows for more than one independent variable to predict a dependent variable. It has the form:\n",
    "\n",
    "y = b0 + b1x1 + b2x2 + ... + bnxn + e\n",
    "\n",
    "where y is the dependent variable, x1, x2, ..., xn are the independent variables, b0, b1, b2, ..., bn are the coefficients, and e is the error term⁵. It differs from simple linear regression in that it can capture more complex relationships and interactions among multiple variables.\n",
    "\n",
    "Q6. Multicollinearity is a situation where two or more independent variables in a multiple linear regression model are highly correlated with each other, meaning that they provide redundant information about the dependent variable⁵. This can cause problems such as:\n",
    "\n",
    "- Inflating the variance of the coefficients, making them unstable and unreliable.\n",
    "- Making it difficult to interpret the individual effects of each variable on the dependent variable.\n",
    "- Reducing the predictive power of the model.\n",
    "\n",
    "You can detect multicollinearity by using methods such as:\n",
    "\n",
    "- Calculating the correlation matrix or scatterplot matrix of the independent variables to check for high correlations.\n",
    "- Calculating the variance inflation factor (VIF) or tolerance of each independent variable to measure how much it is affected by multicollinearity.\n",
    "- Performing principal component analysis (PCA) or factor analysis to reduce the dimensionality of the independent variables.\n",
    "\n",
    "You can address multicollinearity by using methods such as:\n",
    "\n",
    "- Dropping one or more of the highly correlated variables from the model.\n",
    "- Combining two or more of the highly correlated variables into a single variable (such as taking their average or creating an index).\n",
    "- Applying regularization techniques such as ridge regression or lasso regression that penalize large coefficients.\n",
    "\n",
    "Q7. Polynomial regression is a type of regression that fits a polynomial function to the data instead of a linear function. It has the form:\n",
    "\n",
    "y = b0 + b1x + b2x2 + ... + bnxn + e\n",
    "\n",
    "where y is the dependent variable, x is the independent variable, b0, b1, b2, ..., bn are the coefficients, and e is\n",
    "the error term⁵. It differs from linear regression in that it can capture nonlinear relationships and curvature in\n",
    "the data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
