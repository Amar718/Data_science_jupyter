{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1. Lasso Regression is a type of linear regression that performs both variable selection and regularization by adding a penalty term to the loss function that shrinks the coefficients of some features to zero¹. This differs from other regression techniques that either do not perform variable selection (such as ordinary least squares) or use different penalty terms (such as ridge regression).\n",
    "\n",
    "Q2. The main advantage of using Lasso Regression in feature selection is that it can reduce the complexity and improve the interpretability of the model by eliminating irrelevant or redundant features²³.\n",
    "\n",
    "Q3. The coefficients of a Lasso Regression model represent the strength and direction of the relationship between each feature and the outcome variable, after accounting for the effect of other features. The coefficients that are shrunk to zero indicate that the corresponding features are not important for the model²⁴.\n",
    "\n",
    "Q4. The tuning parameter that can be adjusted in Lasso Regression is lambda (λ), which controls the amount of regularization applied to the model. A larger value of λ leads to more shrinkage and sparsity, while a smaller value of λ leads to less shrinkage and sparsity. The optimal value of λ can be chosen by using cross-validation or other methods to minimize the prediction error²⁴.\n",
    "\n",
    "Q5. Yes, Lasso Regression can be used for non-linear regression problems by using basis functions or kernels to transform the input features into a higher-dimensional space, where a linear model can be fitted with lasso regularization⁴.\n",
    "\n",
    "Q6. The difference between Ridge Regression and Lasso Regression is that ridge regression uses an L2 penalty term (the sum of squared coefficients) while lasso regression uses an L1 penalty term (the sum of absolute coefficients). This makes ridge regression shrink all coefficients by the same proportion, while lasso regression shrinks some coefficients to zero and others less so¹⁴.\n",
    "\n",
    "Q7. Yes, Lasso Regression can handle multicollinearity in the input features by selecting one feature from a group of highly correlated features and setting the others to zero, thus reducing the variance and improving the stability of the model¹³.\n",
    "\n",
    "Q8. As mentioned in Q4, the optimal value of lambda (λ) in Lasso Regression can be chosen by using cross-validation or other methods to minimize the prediction error. One common method is to plot the mean squared error (MSE) against different values of λ and choose the value that corresponds to the lowest point on the curve or within one standard error of it²⁴.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
