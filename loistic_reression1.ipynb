{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic regression is a statistical model that is used to predict the probability of a binary outcome (such as yes/no, 0/1, etc.) based on one or more predictor variables. Here are some of the topics that you asked about:\n",
    "\n",
    "The difference between linear regression and logistic regression is that linear regression assumes a continuous outcome variable, while logistic regression assumes a categorical outcome variable. Linear regression models the relationship between the outcome and the predictors using a straight line, while logistic regression models the relationship using a logistic function (also called a sigmoid function) that produces an S-shaped curve. An example of a scenario where logistic regression would be more appropriate is predicting whether a student will be admitted to a university based on their GRE score, GPA, and rank12.\n",
    "The cost function used in logistic regression is called the log-likelihood function, which measures how well the model fits the data. The log-likelihood function is maximized by finding the optimal values of the coefficients that make the predicted probabilities as close as possible to the observed outcomes. This can be done using various optimization algorithms, such as gradient descent, Newton-Raphson, or iteratively reweighted least squares234.\n",
    "Regularization in logistic regression is a technique that adds a penalty term to the cost function to reduce the complexity of the model and prevent overfitting. Overfitting occurs when the model learns too much from the noise or irrelevant features in the data and performs poorly on new or unseen data. Regularization helps to avoid overfitting by shrinking the coefficients of the model towards zero, which reduces their influence on the outcome. There are two common types of regularization: L1 regularization (also called lasso) and L2 regularization (also called ridge). L1 regularization tends to produce sparse models with some coefficients being exactly zero, while L2 regularization tends to produce more balanced models with smaller coefficients34."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
