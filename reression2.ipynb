{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1. **R-squared** is a measure of how well the linear regression fits the data. It is calculated as the ratio of the explained variance to the total variance of the dependent variable. It represents the percentage of the variance in the dependent variable that the independent variables explain collectively¹²³.\n",
    "\n",
    "Q2. **Adjusted R-squared** is a modified version of R-squared that takes into account the number of predictors in the model. It is calculated by multiplying R-squared by a factor that penalizes the model for adding more predictors. It differs from the regular R-squared in that it can decrease when adding more predictors that do not improve the model fit⁴.\n",
    "\n",
    "Q3. It is more appropriate to use adjusted R-squared when comparing models with different numbers of predictors, as it can prevent overestimating the model fit due to adding irrelevant predictors⁴.\n",
    "\n",
    "\n",
    "Q4. **RMSE**, **MSE**, and **MAE** are evaluation metrics for regression analysis that measure the difference between the actual values and the predicted values by the regression model. They are calculated as follows:\n",
    "\n",
    "- **RMSE** (Root Mean Squared Error) is the square root of the average of the squared errors.\n",
    "- **MSE** (Mean Squared Error) is the average of the squared errors.\n",
    "- **MAE** (Mean Absolute Error) is the average of the absolute errors.\n",
    "\n",
    "These metrics represent how well the regression model fits the data and how accurate its predictions are. Lower values indicate better performance.¹²³\n",
    "\n",
    "Q5. The advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis are:\n",
    "\n",
    "- RMSE and MSE penalize large errors more than MAE, which makes them more sensitive to outliers and skewed distributions. This can be desirable if large errors are unacceptable, but it can also distort the overall error measure.¹²\n",
    "- RMSE and MSE have the same units as the dependent variable, which makes them easier to interpret than MAE, which has different units.¹²\n",
    "- MAE is more robust to outliers and skewed distributions than RMSE and MSE, which makes it more suitable for data that have extreme values or are not normally distributed.¹²\n",
    "- MAE is easier to calculate than RMSE and MSE, which require taking square roots or squares.¹²\n",
    "\n",
    "Q6. **Lasso regularization** is a technique that adds a penalty term to the cost function of a linear regression model, which is proportional to the sum of the absolute values of the coefficients. This penalty term shrinks some of the coefficients to zero, which effectively performs feature selection and reduces model complexity.⁵⁶\n",
    "\n",
    "Lasso regularization differs from Ridge regularization, which adds a penalty term proportional to the sum of the squares of the coefficients. Ridge regularization shrinks all of the coefficients by the same amount, but does not eliminate any of them.⁵⁶\n",
    "\n",
    "Lasso regularization is more appropriate to use when there are many irrelevant or redundant features in the data, or when interpretability is important.⁵⁶\n",
    "\n",
    "Q7. Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the cost function of a linear regression model, which reduces the magnitude of the coefficients. This prevents the model from fitting too closely to the noise or variance in the data, and improves its generalization ability.⁵⁶\n",
    "\n",
    "An example of regularized linear models is Ridge regression, which adds a penalty term proportional to the sum of the squares of the coefficients. This shrinks all of the coefficients by the same amount, but does not eliminate any of them.⁵⁶\n",
    "\n",
    "Q8. The limitations of regularized linear models are:\n",
    "\n",
    "- They may introduce bias in the model by shrinking or eliminating some of the coefficients, which may affect its accuracy or validity.⁵⁶\n",
    "- They require choosing an optimal value for the regularization parameter, which controls the amount of regularization. This can be done using cross-validation or grid search, but it adds complexity and computational cost to the model selection process.⁵⁶\n",
    "- They may not perform well when there are nonlinear relationships or interactions among the features or between the features and the dependent variable. In such cases, more flexible models such as polynomial regression, decision trees, or neural networks may be more suitable.⁷ [8]\n",
    "\n",
    "Q9. To compare the performance of two regression models using different evaluation metrics, it is important to consider what kind of errors are acceptable or unacceptable for the problem at hand. For example, if large errors are very costly or harmful, then RMSE may be a better metric than MAE, as it penalizes large errors more than MAE. On the other hand, if small errors are not significant or beneficial, then MAE may be a better metric than RMSE, as it does not inflate small errors as much as RMSE.[1][2][3]\n",
    "\n",
    "In this case, Model A has an RMSE of 10, while Model B has an MAE of 8. Assuming that both models have similar ranges of predicted values, this means that Model A has larger errors than Model B on average, but also smaller errors than Model B on some instances. Model B has more consistent errors than Model A on average, but also larger errors than Model A on some instances\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
