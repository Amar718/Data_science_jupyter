{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV is a technique used in machine learning to search and find the optimal combination of hyperparameters for a given model. It systematically explores a predefined set of hyperparameter values, creating a “grid” of possible combinations123. You can use the sklearn.model_selection.GridSearchCV function from the scikit-learn library to implement this technique45. Hyperparameters are parameters that are not learned by the model, but affect its performance and behavior. For example, the number of hidden layers in a neural network, or the learning rate in a gradient descent algorithm.\n",
    "\n",
    "\n",
    "Q3. **Data leakage** is when information from outside the training dataset is used to create the model⁵. This can cause the model to overfit the training data and perform poorly on new data². An example of data leakage is using future data or irrelevant data in the training set, such as the outcome of an event or the customer ID³⁴.\n",
    "\n",
    "Q4. To prevent data leakage, you should follow these steps:\n",
    "- Split your data into training, validation and test sets before doing any preprocessing or feature engineering⁵.\n",
    "- Apply any transformations or scaling to the data separately for each set, using only the statistics from the training set⁵.\n",
    "- Avoid using any features that are not available at prediction time, such as future data or target values²³.\n",
    "- Check for duplicate or overlapping rows between the sets and remove them if needed².\n",
    "\n",
    "Q5. A **confusion matrix** is a table that shows how well a classification model performs on a set of test data for which the true values are known. It compares the actual labels with the predicted labels and counts the number of correct and incorrect predictions for each class.\n",
    "\n",
    "Q6. **Precision** is the ratio of true positives (TP) to all predicted positives (TP + FP), where FP is false positives. It measures how accurate the model is in identifying positive cases. **Recall** is the ratio of true positives (TP) to all actual positives (TP + FN), where FN is false negatives. It measures how sensitive the model is in finding positive cases.\n",
    "\n",
    "Q7. You can interpret a confusion matrix by looking at the diagonal elements, which show the number of correct predictions for each class, and the off-diagonal elements, which show the number of incorrect predictions for each class. You can also calculate the precision and recall for each class and compare them to see which types of errors your model is making more often.\n",
    "\n",
    "Q8. Some common metrics that can be derived from a confusion matrix are:\n",
    "- **Accuracy**: The ratio of correct predictions to all predictions. It is calculated as (TP + TN) / (TP + TN + FP + FN), where TN is true negatives.\n",
    "- **F1-score**: The harmonic mean of precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall). It balances both precision and recall and gives more weight to low values.\n",
    "- **ROC curve**: A plot of the true positive rate (recall) versus the false positive rate (1 - specificity), where specificity is the ratio of true negatives (TN) to all actual negatives (TN + FP). It shows how well the model can distinguish between positive and negative classes at different thresholds.\n",
    "- **AUC**: The area under the ROC curve. It measures how well the model can rank positive cases higher than negative cases. A higher AUC means a better model performance.\n",
    "\n",
    "Q9. The accuracy of a model is directly related to the values in its confusion matrix. If the confusion matrix has high values on the diagonal and low values on the off-diagonal, it means that the model has a high accuracy. If the confusion matrix has low values on the diagonal and high values on the off-diagonal, it means that the model has a low accuracy.\n",
    "\n",
    "Q10. You can use a confusion matrix to identify potential biases or limitations in your machine learning model by analyzing its performance on different classes or groups of data. For example, you can check if your model has a high precision but a low recall for a certain class, which means that it is missing many positive cases for that class. Or you can check if your model has a low accuracy for a certain group of data, which means that it is not generalizing well to that group.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
