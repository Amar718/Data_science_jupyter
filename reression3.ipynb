{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1. Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated¹. It differs from ordinary least squares regression by adding a penalty term to the cost function, which shrinks the coefficients and reduces overfitting³.\n",
    "\n",
    "Q2. The assumptions of ridge regression are similar to those of linear regression, such as linearity, homoscedasticity, independence and normality of errors. Additionally, ridge regression assumes that there is some multicollinearity among the independent variables².\n",
    "\n",
    "Q3. The value of the tuning parameter (lambda) in ridge regression determines how much the coefficients are shrunk towards zero. A larger lambda means more shrinkage and more bias, but less variance and less overfitting. A smaller lambda means less shrinkage and less bias, but more variance and more overfitting. The optimal value of lambda can be found by using cross-validation or other methods⁴.\n",
    "\n",
    "Q4. Ridge regression can be used for feature selection, but not in a direct way. Ridge regression does not set any coefficient to exactly zero, but it reduces their magnitude depending on their importance. Therefore, ridge regression can be used to identify the most relevant features by ranking them according to their absolute coefficient values³.\n",
    "\n",
    "Q5. Ridge regression performs well in the presence of multicollinearity, as it reduces the variance of the coefficient estimates and improves their stability. Ridge regression can also handle ill-conditioned matrices that arise from multicollinearity, as it adds a positive value to the diagonal elements of the matrix².\n",
    "\n",
    "Q6. Ridge regression can handle both categorical and continuous independent variables, as long as they are properly encoded and scaled. For categorical variables, dummy coding or one-hot encoding can be used to create binary indicators. For continuous variables, standardization or normalization can be used to bring them to a common scale².\n",
    "\n",
    "Q7. The coefficients of ridge regression can be interpreted in a similar way as those of linear regression, but with some caution. The coefficients represent the change in the dependent variable for a unit change in the independent variable, holding all other variables constant. However, the coefficients are biased and depend on the value of lambda, so they cannot be directly compared across different models or datasets².\n",
    "\n",
    "Q8. Ridge regression can be used for time-series data analysis, but with some modifications. Time-series data often have serial correlation, non-stationarity and seasonality, which violate some of the assumptions of ridge regression. Therefore, ridge regression should be applied after transforming the data to make it stationary, removing any trends or cycles, and accounting for any lagged effects or autocorrelation².\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
